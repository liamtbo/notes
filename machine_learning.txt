gradient clipping
    used during the training of deep neural networks to prevent the exploding gradient problem
exploding gradients
    very deep nn runs the highest risk
    gradients become very large as parameters are multiplied through forward pass
vanishing gradients
    opposite of exploding

large batch size (small if the opposite)
    generalization issues
        overfitting
            lower variance in gradient est., but can result in overfitting
        poor generalization 
            since less noisy gradient creates smoother optimization landscape
            harder to escape sharp minima
    learning dynamics
        diminished regularization
            bc less grad noise
        slower exploration
            less frequent updates to model params so slower exploration param space slower
    computational and memory contraints
        resource intensice
            requires more memory
        diminished returns
            increasing batch size eventually reduces benefits of reduced grad noise
    learning rate and training stability
        learning rate sensitity - requires larger lr to maintain efficient training progress
    
            
    reduced noise in gradient estimation
    increase in overfitting bc it makes optimization landscape smoother
    potentially making it easier for the model to settle into sharp minima = poorer generalization

small batch size
    opposite of large batch size

sharp minima
    Points in the loss landscape of a neural network where the loss function has a steep curvature.
    These minimizers are typically associated with narrow, deep valleys in the loss landscape.
    less stable - small changes in model params or input can lead to large increases in loss
    associated with overfitting 
        inputs change (like validation data) can lead to small movement in valley = large loss

flat minima
    opposite of sharp minima

policy gradient methods
    class of algos in RL used to directly optimize the policy
    not a value function like DQN
    useful for env with contrinuous action spaces

regression task
    predicting a continuous value based on inputs
batch gradient descent
    using whole dataset to compute gradient
stocastic gradient descent
    using a small batch to comppute gradient

Actor-critic
    Policy (actor)
        returns action based on policy and state
    Value function (critic)
        returns state and state-action value
    A(s,a) advantage function
        A(s,a) = Q(s,a) - V(s)

rl algos
    Base Actor critic
    A2C
    A3C
    Proximal Policu Optimization (PPO)
    DQN
    Sarsa
    Expected Sarsa
    dyna-Q (+)
    DDPG

Advantage Actor-Critic A2C
    actor (policy) gradient
        ∇θ​J(θ)≈N1​∑i=0N​∇θ​logπθ​(ai​∣si​)⋅A(si​,ai​)
    critic (value) gradient
        ∇w​J(w)≈N1​∑i=1N​∇w​(Vw​(si​)−Qw​(si​,ai​))2

PPO
    actor-critic frameworks
    use log prob instead of raw prob - more stable
    actor (policy) network update
        r(θ)= importance weights = π'θ​(a∣s) / πθ​(a∣s)​​
        L_ppo​(θ)=E[min(r(θ)⋅A(s,a),clip(r(θ),1−ϵ,1+ϵ)⋅A(s,a))]
    critic (value) network update
        Lcritic​(ϕ)=E[(Vϕ​(s)−Rt​)2]

non-linearity
    functions
        tanh
        ReLU
activation functions
    reLU
    tanh
        non-linearity and activation
            allows neural network to learn more complex functions
            without non-linearility -> nn will be a linear model
        output range control
            bounds outputs between [-1,1] - keeps values from growing too large or small (instability)
            symmetry around zero - helps when action space is continious and can be pos/neg
        gradient propagation
            suffers from vanishing gradient problem in deep nn
            in moderate nn's, it can ensure grads aren't too large or too small
        network regularization
            smoothing effect on networks outputs
    

neural networks
    data should always be normalized (distribution created) before being fed to a neural network

    tanh-normal distribution
        normal dis output is ran through tanh function
        used in continuous action spaces, to ensure that the action values remain within 
        a specific range.
        adds non-linearity