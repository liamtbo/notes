tensors
    creating tensors
        essentially numpy array/matrix that can run on GPUs
        x_data = torch.tensor([[1,2],[3,4]])
        x_np = torch.from_numpy(np_array)
        x_ones = torch.ones_like(x_data) # shape of x_data but all ones
        x_rand = torch.rand_like(x_data, dtype=torch.float) # shape of x_data but all random
        rand_tensor = torch.rand(shape)
        ones_tensor = torch.ones(shape)
        zeros_tensor = torch.zeros(shape)
    attributes
        tensor.shape
        tensor.dtype
        tensor.devices
    operations
        standard.
    move tensor to GPU instead of CPU
        if torch.cuda.is_available():
            tensor = tensor.to('cuda')
    indexing & slicing
        tensor = torch.ones(4, 4)
        print('First row: ',tensor[0])
        print('First column: ', tensor[:, 0])
        print('Last column:', tensor[..., -1])
        tensor[:,1] = 0
    concatenation
        t1 = torch.cat([tensor1, tensor2, tensor3], dim=1) # dim concats rows of each tensor matrix
    matrix mult
        y2 = tensor.matmul(tensor.T)
    element-wise
        z1 = tensor * tensor
    single element tensor to python number value
        tensor.item()

Loss functions
    nn.SmoothL1loss(predicted, actual)
        acts like the mean squared error when the error is small, but like the 
        mean absolute error when the error is large - this makes it more robust 
        to outliers when the estimates of QQ are very noisy

