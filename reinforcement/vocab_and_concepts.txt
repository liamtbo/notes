advantage function
    A(st​,at​)=Q(st​,at​)−V(st​)
    a measure of how much brtter or worse and action a_t is compared to the average action the policy would take in s_t
baseline
    ∇θ​J(θ)=Eπθ​​[t=0∑T​∇θ​logπθ​(at​∣st​)⋅(Rt​−b(st​))]
    value that is subtracted from the return or reward when calculating the policy gradient.
    When baseline is the value function, it's the same as the advantage function

gradient clipping
    used during the training of deep neural networks to prevent the exploding gradient problem
exploding gradients
    very deep nn runs the highest risk
    gradients become very large as parameters are multiplied through forward pass
vanishing gradients
    opposite of exploding
value-based methods
    focus on estimating the value of states or actions to derive the optimal policy.
    The value function represents the expected cumulative rewards that can be obtained
    from a state or state-action pair.
policy-based methods
    policy based methods direclty learn the policy that maps states to actions without
    explicitly estimatiing value functions. The policy is parameterized and optimized
    to maximize the expected rewards.


large batch size (small if the opposite)
    generalization issues
        overfitting
            lower variance in gradient est., but can result in overfitting
        poor generalization 
            since less noisy gradient creates smoother optimization landscape
            harder to escape sharp minima
    learning dynamics
        diminished regularization
            bc less grad noise
        slower exploration
            less frequent updates to model params so slower exploration param space slower
    computational and memory contraints
        resource intensice
            requires more memory
        diminished returns
            increasing batch size eventually reduces benefits of reduced grad noise
    learning rate and training stability
        learning rate sensitity - requires larger lr to maintain efficient training progress
    
            
    reduced noise in gradient estimation
    increase in overfitting bc it makes optimization landscape smoother
    potentially making it easier for the model to settle into sharp minima = poorer generalization

small batch size
    opposite of large batch size

sharp minima
    Points in the loss landscape of a neural network where the loss function has a steep curvature.
    These minimizers are typically associated with narrow, deep valleys in the loss landscape.
    less stable - small changes in model params or input can lead to large increases in loss
    associated with overfitting 
        inputs change (like validation data) can lead to small movement in valley = large loss

flat minima
    opposite of sharp minima





policy gradient methods
    class of algos in RL used to directly optimize the policy
    not a value function like DQN
    useful for env with contrinuous action spaces

regression task
    predicting a continuous value based on inputs
batch gradient descent
    using whole dataset to compute gradient
stocastic gradient descent
    using a small batch to comppute gradient

numerical instability
    occus when small changes in input or operations lead to large and unpredictable
    changes in the output of a computation

location scaling (loc scaling)
    scaling loc(mean) to reduce numerical instability
    ex
        moving loc of dis closer to 0 st that when tanh is applied, the output isn't
        -1 or 1 (avoiding gradient vanishing/explosion)


non-linearity
    functions
        tanh
        ReLU
        
activation functions
    reLU
    tanh
        applies tanh to a normal dis output
        for outputting action that are both bounded and continuous
        used with loc scaling to reduce numerical instability
        non-linearity and activation
            allows neural network to learn more complex functions
            without non-linearility -> nn will be a linear model
        output range control
            bounds outputs between (-1,1) - keeps values from growing too large or small (instability)
            symmetry around zero - helps when action space is continious and can be pos/neg
        gradient propagation
            suffers from vanishing gradient problem in deep nn
            in moderate nn's, it can ensure grads aren't too large or too small
        network regularization
            smoothing effect on networks outputs
    

neural networks
    data should always be normalized (distribution created) before being fed to a neural network

    tanh-normal distribution
        normal dis output is ran through tanh function
        used in continuous action spaces, to ensure that the action values remain within 
        a specific range.
        adds non-linearity

entropy (of policy)
    measure of randomness or uncertainty in the actions taken by a policy
entropy bonuses
    bonuses added to agents objective function to encourage a higher level of randomness in action taking

intrinsic reward
    rewards given by agent to iteslf to encourage exploration
episdic intrinsic rewards
    rewards given at the end of an epsiode based on agents exploration behavior during the episode. Its meant to encourage the agent to explorer new or less frequently visited states
forward dynamic model
    model predicts next state of env given current state and action
reward model
    predicts the reward associated with a given state and action

loss
    huber loss (smooth_l1)
        behaves like Mean Squared Error (MSE) for small differences and like 
        Mean Absolute Error (MAE) for larger differences. This combination makes 
        it more robust to outliers than MSE while being smoother than MAE, which 
        can be beneficial in training stability.

MARL
    non-centralized environment
        each agents policy can change over time as it learns, leading to a non-stationary env from the perspective and of individual agent. 
    decentralised execution
        policy for a single agent will output an action for that egent based only on its observation

    decentralized critic
        each agents critic evaluates the value of actions based only on the local observations and the actions of that agent.
        Can suffer from learning instability due to the non-stationarity introduced by other agents policies
        Useful when agents need to act bsed on limited info, or when systems scale makes centralized approaches impractical
    centralied critic
        helps mitigate the issue of non-stationarity in MARL
        Each agent has a critic that has access to the global state and the actions of all agents.
        Allows ciritc to evaluate the value of action more accurately
        Agents will learn more stable and consistent value functions.
        During execution, the policies become decentralised
    
    frameworks for critic
        MAPPO
            critic is centralized and take as input the global state of the system
        IPPO
            critic is decentralised
            critic takes as input the observation of the respective agent, exactly like the policy. 

    vectorization
        process of running multiple instances of an env in parallel