random forest (decision trees)
    1. create a bootstrapped dataset
    2. create a decision tree using the bootstrapped dataset, but only use a random subset of variable (or columns) at each step. Example is picking two randomm columns, seeing which one does better at predicting, choosing that col
    3. go back to step 1 and repeat (100's of times)
    4. bootstrap sample data to train decision trees
    5. use out-of-bag samples to test random forest
    6. after running data down trees in random forest we see which option (yes/no) had more votes
    7. compare results of steps 1-6 with forests built with diff number of vairables used per step

decision trees
    tree-like model used for making decision or predictions

classification trees
    classifies things into categories
    left if statement true else right 

    1. pick root node feature (col) with lowest gini impurity/weighted calculation
    2. pick left node feature by taking samples that said True to root node (bc left node) and passing those into the remaining features. Out of those choose feature with lowest gini impurity as the root->left node
    3. do same thing with root->right node
    4. do recursively until all leaves are pure
    5. output values for each leaf node are the majority

regression trees
    predicts numeric values

clipping
    when leaf hasn't had enough samples for confidence, so remove it

bagging
    bootstrapping the data plus using the aggregate to make a decision

bootstrapped dataset
    random samples from original dataset with replacement (copies)

out-of-bag dataset
    data that was not included in the bootstrapped dataset

impure
    refers to the measure of how mixed or heterogeneous the target class labels are in a given node. Quantifies how well a node can seperate the classifies
    ex: gini impurity, entropy, information gain